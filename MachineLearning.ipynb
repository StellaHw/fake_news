{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MachineLearning.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1A414bLq1bgzrYu5GWlVZ2ljGTVHSmxzM",
      "authorship_tag": "ABX9TyPOFB3zk7+BxVjG1EGpXtSw",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/StellaHw/fake_news_detection/blob/main/MachineLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pWlpGLK0rQP"
      },
      "source": [
        "cd drive/My Drive/MACHINE LEARNING"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2W4tef1b7yZN"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6SAKLJ22WWh"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "dataset = pd.read_excel('Constraint_English_Train.xlsx')\n",
        "datatest = pd.read_excel('Constraint_English_Val.xlsx')\n",
        "dataset.loc[:,'label'] = dataset.label.map({'real':0, 'fake':1})\n",
        "datatest.loc[:,'label'] = datatest.label.map({'real':0, 'fake':1})\n",
        "Y_train = dataset.iloc[:,-1]\n",
        "Y_Val = datatest.iloc[:,-1]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tp0ztTlUT8N-"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_hBsnRFYwc1"
      },
      "source": [
        "import re\n",
        "from gensim.parsing.preprocessing import strip_non_alphanum\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "corpus_train = []\n",
        "for tweet in dataset.values[:,1]:\n",
        "  tweet = re.sub('http\\S+', '', tweet) \n",
        "  tweet = re.sub('[^a-zA-Z0-9]', ' ', tweet)\n",
        "  tweet = strip_non_alphanum(tweet).lower().strip() \n",
        "  tweet = tweet.split() \n",
        "  ps = PorterStemmer()\n",
        "  tweet = [ps.stem(word) for word in tweet if word not in stopwords.words(\"english\")]\n",
        "  tweet = \" \".join(tweet)\n",
        "  corpus_train.append(tweet)\n",
        "\n",
        "corpus_val = []\n",
        "for tweet in datatest.values[:,1]:\n",
        "  tweet = re.sub('http\\S+', '', tweet) \n",
        "  tweet = re.sub('[^a-zA-Z0-9]', ' ', tweet)\n",
        "  tweet = strip_non_alphanum(tweet).lower().strip() \n",
        "  tweet = tweet.split() \n",
        "  ps = PorterStemmer()\n",
        "  tweet = [ps.stem(word) for word in tweet if word not in stopwords.words(\"english\")]\n",
        "  tweet = \" \".join(tweet)\n",
        "  corpus_val.append(tweet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQ1yg_qp5y2p"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tf_idf = TfidfVectorizer()\n",
        "X_train = tf_idf.fit_transform(corpus_train).toarray()\n",
        "max = tf_idf.vocabulary_\n",
        "tf_idf = TfidfVectorizer(vocabulary=max)\n",
        "X_Val = tf_idf.fit_transform(corpus_val).toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urA6MUMACPZE"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "mnb = MultinomialNB()\n",
        "mnb.fit(X_train, Y_train)\n",
        "y_pred = mnb.predict(X_Val)\n",
        "print('Multinomial:')\n",
        "print('accuracy = ', accuracy_score(Y_Val, y_pred))\n",
        "print('f1_score = ', f1_score(Y_Val, y_pred))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FM9_rPmSXbj6"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "lr = LogisticRegression()\n",
        "lr.fit(X_train, Y_train)\n",
        "y_pred = lr.predict(X_Val)\n",
        "\n",
        "print('Logistic Regression:')\n",
        "print('accuracy = ', accuracy_score(Y_Val, y_pred))\n",
        "print('f1_score = ', f1_score(Y_Val, y_pred))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sk-pQWFMVO84"
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "model = SVC(kernel = 'rbf')\n",
        "model.fit(X_train, Y_train)\n",
        "y_pred = model.predict(X_Val)\n",
        "\n",
        "print('SVM kernel rbf :')\n",
        "print('accuracy = ', accuracy_score(Y_Val, y_pred))\n",
        "print('f1_score = ', f1_score(Y_Val, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}